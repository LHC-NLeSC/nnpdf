{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cceb1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from validphys.loader import _get_nnpdf_profile\n",
    "from validphys.api import API\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from validphys.convolution import central_predictions\n",
    "\n",
    "profile = _get_nnpdf_profile()\n",
    "#yaml_db = Path(profile[\"data_path\"]) / \"yamldb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1eb8f3",
   "metadata": {},
   "source": [
    "The `yaml_db` folder is a temporary thing as it contains files that look like:\n",
    "\n",
    "```yaml\n",
    "conversion_factor: 1.0\n",
    "operands:\n",
    "- - NMC_NC_EM_D_F2\n",
    "- - NMC_NC_EM_P_F2\n",
    "operation: RATIO\n",
    "target_dataset: NMCPD\n",
    "```\n",
    "\n",
    "This information will eventually be part of the new commondata format of course.\n",
    "\n",
    "The `operation` is applied to the first level of the list while the second level is just concatenated. This is necessary since `pineappl` fktables might contain one layer of concatenation which is already done for the \"classic\" fktables.\n",
    "\n",
    "The `pineappl` fktables will live inside the appropiate `theory_xxx` folder `/pineappls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051581e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMCPD_dw_ite\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: NMCPD_dw_ite\n",
      "nData: 260 nSys: 105\n",
      "-- COMMONDATA Files for NMCPD_dw_ite successfully read.\n",
      "\n",
      "NMC\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: NMC\n",
      "nData: 292 nSys: 16\n",
      "-- COMMONDATA Files for NMC successfully read.\n",
      "\n",
      "SLACP_dwsh\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: SLACP_dwsh\n",
      "nData: 211 nSys: 3\n",
      "-- COMMONDATA Files for SLACP_dwsh successfully read.\n",
      "\n",
      "SLACD_dw_ite\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: SLACD_dw_ite\n",
      "nData: 211 nSys: 103\n",
      "-- COMMONDATA Files for SLACD_dw_ite successfully read.\n",
      "\n",
      "BCDMSP_dwsh\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: BCDMSP_dwsh\n",
      "nData: 351 nSys: 11\n",
      "-- COMMONDATA Files for BCDMSP_dwsh successfully read.\n",
      "\n",
      "BCDMSD_dw_ite\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: BCDMSD_dw_ite\n",
      "nData: 254 nSys: 108\n",
      "-- COMMONDATA Files for BCDMSD_dw_ite successfully read.\n",
      "\n",
      "CHORUSNUPb_dw_ite\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CHORUSNUPb_dw_ite\n",
      "nData: 607 nSys: 1014\n",
      "-- COMMONDATA Files for CHORUSNUPb_dw_ite successfully read.\n",
      "\n",
      "CHORUSNBPb_dw_ite\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CHORUSNBPb_dw_ite\n",
      "nData: 607 nSys: 114\n",
      "-- COMMONDATA Files for CHORUSNBPb_dw_ite successfully read.\n",
      "\n",
      "NTVNUDMNFe_dw_ite\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: NTVNUDMNFe_dw_ite\n",
      "nData: 45 nSys: 1003\n",
      "-- COMMONDATA Files for NTVNUDMNFe_dw_ite successfully read.\n",
      "\n",
      "NTVNBDMNFe_dw_ite\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: NTVNBDMNFe_dw_ite\n",
      "nData: 45 nSys: 103\n",
      "-- COMMONDATA Files for NTVNBDMNFe_dw_ite successfully read.\n",
      "\n",
      "HERACOMBNCEM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: HERACOMBNCEM\n",
      "nData: 159 nSys: 170\n",
      "-- COMMONDATA Files for HERACOMBNCEM successfully read.\n",
      "\n",
      "HERACOMBNCEP460\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: HERACOMBNCEP460\n",
      "nData: 209 nSys: 170\n",
      "-- COMMONDATA Files for HERACOMBNCEP460 successfully read.\n",
      "\n",
      "HERACOMBNCEP575\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: HERACOMBNCEP575\n",
      "nData: 260 nSys: 170\n",
      "-- COMMONDATA Files for HERACOMBNCEP575 successfully read.\n",
      "\n",
      "HERACOMBNCEP820\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: HERACOMBNCEP820\n",
      "nData: 112 nSys: 170\n",
      "-- COMMONDATA Files for HERACOMBNCEP820 successfully read.\n",
      "\n",
      "HERACOMBNCEP920\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: HERACOMBNCEP920\n",
      "nData: 485 nSys: 170\n",
      "-- COMMONDATA Files for HERACOMBNCEP920 successfully read.\n",
      "\n",
      "HERACOMBCCEM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: HERACOMBCCEM\n",
      "nData: 42 nSys: 170\n",
      "-- COMMONDATA Files for HERACOMBCCEM successfully read.\n",
      "\n",
      "HERACOMBCCEP\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: HERACOMBCCEP\n",
      "nData: 39 nSys: 170\n",
      "-- COMMONDATA Files for HERACOMBCCEP successfully read.\n",
      "\n",
      "HERACOMB_SIGMARED_C\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: HERACOMB_SIGMARED_C\n",
      "nData: 52 nSys: 167\n",
      "-- COMMONDATA Files for HERACOMB_SIGMARED_C successfully read.\n",
      "\n",
      "HERACOMB_SIGMARED_B\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: HERACOMB_SIGMARED_B\n",
      "nData: 27 nSys: 167\n",
      "-- COMMONDATA Files for HERACOMB_SIGMARED_B successfully read.\n",
      "\n",
      "CDFZRAP_NEW\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CDFZRAP_NEW\n",
      "nData: 28 nSys: 11\n",
      "-- COMMONDATA Files for CDFZRAP_NEW successfully read.\n",
      "\n",
      "D0ZRAP_40\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: D0ZRAP_40\n",
      "nData: 28 nSys: 1\n",
      "-- COMMONDATA Files for D0ZRAP_40 successfully read.\n",
      "\n",
      "ATLASWZRAP36PB\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASWZRAP36PB\n",
      "nData: 30 nSys: 32\n",
      "-- COMMONDATA Files for ATLASWZRAP36PB successfully read.\n",
      "\n",
      "ATLASZHIGHMASS49FB\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASZHIGHMASS49FB\n",
      "nData: 13 nSys: 11\n",
      "-- COMMONDATA Files for ATLASZHIGHMASS49FB successfully read.\n",
      "\n",
      "ATLASLOMASSDY11EXT\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASLOMASSDY11EXT\n",
      "nData: 6 nSys: 8\n",
      "-- COMMONDATA Files for ATLASLOMASSDY11EXT successfully read.\n",
      "\n",
      "ATLASWZRAP11CC\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASWZRAP11CC\n",
      "nData: 46 nSys: 133\n",
      "-- COMMONDATA Files for ATLASWZRAP11CC successfully read.\n",
      "\n",
      "ATLASWZRAP11CF\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASWZRAP11CF\n",
      "nData: 15 nSys: 133\n",
      "-- COMMONDATA Files for ATLASWZRAP11CF successfully read.\n",
      "\n",
      "ATLASDY2D8TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASDY2D8TEV\n",
      "nData: 48 nSys: 37\n",
      "-- COMMONDATA Files for ATLASDY2D8TEV successfully read.\n",
      "\n",
      "ATLAS_DY_2D_8TEV_LOWMASS\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_DY_2D_8TEV_LOWMASS\n",
      "nData: 84 nSys: 277\n",
      "-- COMMONDATA Files for ATLAS_DY_2D_8TEV_LOWMASS successfully read.\n",
      "\n",
      "ATLAS_WZ_TOT_13TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_WZ_TOT_13TEV\n",
      "nData: 3 nSys: 4\n",
      "-- COMMONDATA Files for ATLAS_WZ_TOT_13TEV successfully read.\n",
      "\n",
      "ATLAS_WP_JET_8TEV_PT\n",
      "ATLAS_WM_JET_8TEV_PT\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_WM_JET_8TEV_PT\n",
      "nData: 16 nSys: 124\n",
      "-- COMMONDATA Files for ATLAS_WM_JET_8TEV_PT successfully read.\n",
      "\n",
      "ATLASZPT8TEVMDIST\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASZPT8TEVMDIST\n",
      "nData: 64 nSys: 102\n",
      "-- COMMONDATA Files for ATLASZPT8TEVMDIST successfully read.\n",
      "\n",
      "ATLASZPT8TEVYDIST\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASZPT8TEVYDIST\n",
      "nData: 120 nSys: 102\n",
      "-- COMMONDATA Files for ATLASZPT8TEVYDIST successfully read.\n",
      "\n",
      "ATLASTTBARTOT7TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASTTBARTOT7TEV\n",
      "nData: 1 nSys: 3\n",
      "-- COMMONDATA Files for ATLASTTBARTOT7TEV successfully read.\n",
      "\n",
      "ATLASTTBARTOT8TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASTTBARTOT8TEV\n",
      "nData: 1 nSys: 3\n",
      "-- COMMONDATA Files for ATLASTTBARTOT8TEV successfully read.\n",
      "\n",
      "ATLAS_TTBARTOT_13TEV_FULLLUMI\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_TTBARTOT_13TEV_FULLLUMI\n",
      "nData: 1 nSys: 2\n",
      "-- COMMONDATA Files for ATLAS_TTBARTOT_13TEV_FULLLUMI successfully read.\n",
      "\n",
      "ATLAS_TTB_DIFF_8TEV_LJ_TRAPNORM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_TTB_DIFF_8TEV_LJ_TRAPNORM\n",
      "nData: 5 nSys: 135\n",
      "-- COMMONDATA Files for ATLAS_TTB_DIFF_8TEV_LJ_TRAPNORM successfully read.\n",
      "\n",
      "ATLAS_TTB_DIFF_8TEV_LJ_TTRAPNORM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_TTB_DIFF_8TEV_LJ_TTRAPNORM\n",
      "nData: 5 nSys: 135\n",
      "-- COMMONDATA Files for ATLAS_TTB_DIFF_8TEV_LJ_TTRAPNORM successfully read.\n",
      "\n",
      "ATLAS_TOPDIFF_DILEPT_8TEV_TTRAPNORM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_TOPDIFF_DILEPT_8TEV_TTRAPNORM\n",
      "nData: 5 nSys: 5\n",
      "-- COMMONDATA Files for ATLAS_TOPDIFF_DILEPT_8TEV_TTRAPNORM successfully read.\n",
      "\n",
      "ATLAS_1JET_8TEV_R06_DEC\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_1JET_8TEV_R06_DEC\n",
      "nData: 171 nSys: 677\n",
      "-- COMMONDATA Files for ATLAS_1JET_8TEV_R06_DEC successfully read.\n",
      "\n",
      "ATLAS_2JET_7TEV_R06\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_2JET_7TEV_R06\n",
      "nData: 90 nSys: 474\n",
      "-- COMMONDATA Files for ATLAS_2JET_7TEV_R06 successfully read.\n",
      "\n",
      "ATLASPHT15_SF\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLASPHT15_SF\n",
      "nData: 53 nSys: 2\n",
      "-- COMMONDATA Files for ATLASPHT15_SF successfully read.\n",
      "\n",
      "ATLAS_SINGLETOP_TCH_R_7TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_SINGLETOP_TCH_R_7TEV\n",
      "nData: 1 nSys: 13\n",
      "-- COMMONDATA Files for ATLAS_SINGLETOP_TCH_R_7TEV successfully read.\n",
      "\n",
      "ATLAS_SINGLETOP_TCH_R_13TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_SINGLETOP_TCH_R_13TEV\n",
      "nData: 1 nSys: 1\n",
      "-- COMMONDATA Files for ATLAS_SINGLETOP_TCH_R_13TEV successfully read.\n",
      "\n",
      "ATLAS_SINGLETOP_TCH_DIFF_7TEV_T_RAP_NORM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_SINGLETOP_TCH_DIFF_7TEV_T_RAP_NORM\n",
      "nData: 3 nSys: 17\n",
      "-- COMMONDATA Files for ATLAS_SINGLETOP_TCH_DIFF_7TEV_T_RAP_NORM successfully read.\n",
      "\n",
      "ATLAS_SINGLETOP_TCH_DIFF_7TEV_TBAR_RAP_NORM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_SINGLETOP_TCH_DIFF_7TEV_TBAR_RAP_NORM\n",
      "nData: 3 nSys: 15\n",
      "-- COMMONDATA Files for ATLAS_SINGLETOP_TCH_DIFF_7TEV_TBAR_RAP_NORM successfully read.\n",
      "\n",
      "ATLAS_SINGLETOP_TCH_DIFF_8TEV_T_RAP_NORM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_SINGLETOP_TCH_DIFF_8TEV_T_RAP_NORM\n",
      "nData: 3 nSys: 31\n",
      "-- COMMONDATA Files for ATLAS_SINGLETOP_TCH_DIFF_8TEV_T_RAP_NORM successfully read.\n",
      "\n",
      "ATLAS_SINGLETOP_TCH_DIFF_8TEV_TBAR_RAP_NORM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_SINGLETOP_TCH_DIFF_8TEV_TBAR_RAP_NORM\n",
      "nData: 3 nSys: 31\n",
      "-- COMMONDATA Files for ATLAS_SINGLETOP_TCH_DIFF_8TEV_TBAR_RAP_NORM successfully read.\n",
      "\n",
      "CMSWEASY840PB\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSWEASY840PB\n",
      "nData: 11 nSys: 11\n",
      "-- COMMONDATA Files for CMSWEASY840PB successfully read.\n",
      "\n",
      "CMSWMASY47FB\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSWMASY47FB\n",
      "nData: 11 nSys: 11\n",
      "-- COMMONDATA Files for CMSWMASY47FB successfully read.\n",
      "\n",
      "CMSDY2D11\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSDY2D11\n",
      "nData: 132 nSys: 133\n",
      "-- COMMONDATA Files for CMSDY2D11 successfully read.\n",
      "\n",
      "CMSWMU8TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSWMU8TEV\n",
      "nData: 22 nSys: 45\n",
      "-- COMMONDATA Files for CMSWMU8TEV successfully read.\n",
      "\n",
      "CMSZDIFF12\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSZDIFF12\n",
      "nData: 50 nSys: 52\n",
      "-- COMMONDATA Files for CMSZDIFF12 successfully read.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMS_2JET_7TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMS_2JET_7TEV\n",
      "nData: 54 nSys: 88\n",
      "-- COMMONDATA Files for CMS_2JET_7TEV successfully read.\n",
      "\n",
      "CMS_1JET_8TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMS_1JET_8TEV\n",
      "nData: 239 nSys: 293\n",
      "-- COMMONDATA Files for CMS_1JET_8TEV successfully read.\n",
      "\n",
      "CMSTTBARTOT7TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSTTBARTOT7TEV\n",
      "nData: 1 nSys: 2\n",
      "-- COMMONDATA Files for CMSTTBARTOT7TEV successfully read.\n",
      "\n",
      "CMSTTBARTOT8TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSTTBARTOT8TEV\n",
      "nData: 1 nSys: 2\n",
      "-- COMMONDATA Files for CMSTTBARTOT8TEV successfully read.\n",
      "\n",
      "CMSTTBARTOT13TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSTTBARTOT13TEV\n",
      "nData: 1 nSys: 2\n",
      "-- COMMONDATA Files for CMSTTBARTOT13TEV successfully read.\n",
      "\n",
      "CMSTOPDIFF8TEVTTRAPNORM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSTOPDIFF8TEVTTRAPNORM\n",
      "nData: 10 nSys: 21\n",
      "-- COMMONDATA Files for CMSTOPDIFF8TEVTTRAPNORM successfully read.\n",
      "\n",
      "CMSTTBARTOT5TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMSTTBARTOT5TEV\n",
      "nData: 1 nSys: 2\n",
      "-- COMMONDATA Files for CMSTTBARTOT5TEV successfully read.\n",
      "\n",
      "CMS_TTBAR_2D_DIFF_MTT_TRAP_NORM\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMS_TTBAR_2D_DIFF_MTT_TRAP_NORM\n",
      "nData: 16 nSys: 44\n",
      "-- COMMONDATA Files for CMS_TTBAR_2D_DIFF_MTT_TRAP_NORM successfully read.\n",
      "\n",
      "CMS_TTB_DIFF_13TEV_2016_2L_TRAP\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMS_TTB_DIFF_13TEV_2016_2L_TRAP\n",
      "nData: 10 nSys: 10\n",
      "-- COMMONDATA Files for CMS_TTB_DIFF_13TEV_2016_2L_TRAP successfully read.\n",
      "\n",
      "CMS_TTB_DIFF_13TEV_2016_LJ_TRAP\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMS_TTB_DIFF_13TEV_2016_LJ_TRAP\n",
      "nData: 11 nSys: 11\n",
      "-- COMMONDATA Files for CMS_TTB_DIFF_13TEV_2016_LJ_TRAP successfully read.\n",
      "\n",
      "CMS_SINGLETOP_TCH_TOT_7TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMS_SINGLETOP_TCH_TOT_7TEV\n",
      "nData: 1 nSys: 3\n",
      "-- COMMONDATA Files for CMS_SINGLETOP_TCH_TOT_7TEV successfully read.\n",
      "\n",
      "CMS_SINGLETOP_TCH_R_8TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMS_SINGLETOP_TCH_R_8TEV\n",
      "nData: 1 nSys: 1\n",
      "-- COMMONDATA Files for CMS_SINGLETOP_TCH_R_8TEV successfully read.\n",
      "\n",
      "CMS_SINGLETOP_TCH_R_13TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: CMS_SINGLETOP_TCH_R_13TEV\n",
      "nData: 1 nSys: 1\n",
      "-- COMMONDATA Files for CMS_SINGLETOP_TCH_R_13TEV successfully read.\n",
      "\n",
      "LHCBZ940PB\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: LHCBZ940PB\n",
      "nData: 9 nSys: 11\n",
      "-- COMMONDATA Files for LHCBZ940PB successfully read.\n",
      "\n",
      "LHCBZEE2FB_40\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: LHCBZEE2FB_40\n",
      "nData: 17 nSys: 19\n",
      "-- COMMONDATA Files for LHCBZEE2FB_40 successfully read.\n",
      "\n",
      "LHCBWZMU7TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: LHCBWZMU7TEV\n",
      "nData: 33 nSys: 35\n",
      "-- COMMONDATA Files for LHCBWZMU7TEV successfully read.\n",
      "\n",
      "LHCBWZMU8TEV\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: LHCBWZMU8TEV\n",
      "nData: 34 nSys: 36\n",
      "-- COMMONDATA Files for LHCBWZMU8TEV successfully read.\n",
      "\n",
      "LHCB_Z_13TEV_DIMUON\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: LHCB_Z_13TEV_DIMUON\n",
      "nData: 18 nSys: 19\n",
      "-- COMMONDATA Files for LHCB_Z_13TEV_DIMUON successfully read.\n",
      "\n",
      "LHCB_Z_13TEV_DIELECTRON\n",
      "\n",
      "-- Reading COMMONDATA for Dataset: LHCB_Z_13TEV_DIELECTRON\n",
      "nData: 17 nSys: 18\n",
      "-- COMMONDATA Files for LHCB_Z_13TEV_DIELECTRON successfully read.\n",
      "\n",
      "             vp       pine ratio CMSTTBARTOT5TEV, ['QCD']\n",
      "              0          0                              0\n",
      "data                                                     \n",
      "0     69.137978  63.366976                       1.091073\n",
      "            vp      pine ratio CMS_TTBAR_2D_DIFF_MTT_TRAP_NORM, ['QCD']\n",
      "             0         0                                              0\n",
      "data                                                                   \n",
      "0     0.002605  0.003382                                       0.770101\n",
      "1     0.002313  0.002982                                       0.775740\n",
      "2     0.001639  0.002114                                       0.775299\n",
      "3     0.000570  0.000741                                       0.769081\n",
      "4     0.002545  0.002664                                       0.955411\n",
      "5     0.002176  0.002363                                       0.921011\n",
      "6     0.001513  0.001637                                       0.924156\n",
      "7     0.000536  0.000578                                       0.927862\n",
      "8     0.000998  0.001038                                       0.961663\n",
      "9     0.000857  0.000930                                       0.921232\n",
      "10    0.000624  0.000678                                       0.920042\n",
      "11    0.000254  0.000276                                       0.918816\n",
      "12    0.000072  0.000069                                       1.032299\n",
      "13    0.000064  0.000069                                       0.923609\n",
      "14    0.000059  0.000064                                       0.927297\n",
      "15    0.000032  0.000035                                       0.918087\n"
     ]
    }
   ],
   "source": [
    "# Test them all\n",
    "if True:\n",
    "    from yaml import safe_load\n",
    "    pdf = API.pdf(pdf=\"NNPDF40_nnlo_as_01180\")\n",
    "    all_res = []\n",
    "    # Reference here a NNPDF40 runcard to read up all datasets\n",
    "    #nnpdf40_runcard = safe_load(Path(\"/home/juacrumar/NNPDF-testing/nnpdf/n3fit/NNPDF40_with_pineappl.yml\").read_text())\n",
    "    nnpdf40_runcard = safe_load(Path(\"/mount/storage/Academic_Workspace/NNPDF/src/nnpdf/n3fit/NNPDF40_with_pineappl.yml\").read_text())\n",
    "    for d in nnpdf40_runcard[\"dataset_inputs\"]:\n",
    "        target_ds = d[\"dataset\"]\n",
    "        #if any(skipthis in target_ds for skipthis in [\"HERA\", \"NMC\", \"NTV\", \"CHORUS\", \"SLAC\", \"BCD\"]):\n",
    "        #    continue\n",
    "        print(target_ds)\n",
    "        cfac = d.get(\"cfac\", [])\n",
    "        old_ds = API.dataset(dataset_input={\"dataset\": target_ds, \"cfac\": cfac}, theoryid=200, use_cuts=\"internal\")\n",
    "        ds = API.dataset(dataset_input={\"dataset\": target_ds, \"cfac\": cfac}, theoryid=400, use_cuts=\"internal\")\n",
    "        new_cp = central_predictions(ds, pdf)\n",
    "        cp = central_predictions(old_ds, pdf)\n",
    "        all_res.append(pd.concat([new_cp, cp, new_cp/cp], axis=1, keys=[\"vp\", \"pine\", f\"ratio {target_ds}, {cfac}\"]))\n",
    "        \n",
    "    for i in all_res:\n",
    "        mean_ratio = i[i.columns[2]].mean()\n",
    "        if not (0.95 < mean_ratio < 1.05):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d88771",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ds = \"ATLAS_WP_JET_8TEV_PT\"\n",
    "cfac = [\"QCD\"] # [\"NRM\"]\n",
    "old_ds = API.dataset(dataset_input={\"dataset\": target_ds, \"cfac\": cfac}, theoryid=200, use_cuts=\"internal\")\n",
    "ds = API.dataset(dataset_input={\"dataset\": target_ds, \"cfac\": cfac}, theoryid=400, use_cuts=\"internal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316a571e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_WP_JET_8TEV_PT\n",
      "nData: 16 nSys: 124\n",
      "-- COMMONDATA Files for ATLAS_WP_JET_8TEV_PT successfully read.\n",
      "\n",
      "LHAPDF 6.4.0 loading /usr/share/lhapdf/LHAPDF/NNPDF40_nnlo_as_01180/NNPDF40_nnlo_as_01180_0000.dat\n",
      "NNPDF40_nnlo_as_01180 PDF set, member #0, version 1; LHAPDF ID = 331100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>pine</th>\n",
       "      <th>vp</th>\n",
       "      <th>ratio vp/ratio</th>\n",
       "      <th>ratio pine/vp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5611.439817</td>\n",
       "      <td>5616.248344</td>\n",
       "      <td>1.000857</td>\n",
       "      <td>0.999144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3154.767573</td>\n",
       "      <td>3157.559479</td>\n",
       "      <td>1.000885</td>\n",
       "      <td>0.999116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1194.467638</td>\n",
       "      <td>1195.354244</td>\n",
       "      <td>1.000742</td>\n",
       "      <td>0.999258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>515.187956</td>\n",
       "      <td>515.574565</td>\n",
       "      <td>1.000750</td>\n",
       "      <td>0.999250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>246.767542</td>\n",
       "      <td>246.941654</td>\n",
       "      <td>1.000706</td>\n",
       "      <td>0.999295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>126.644243</td>\n",
       "      <td>126.731587</td>\n",
       "      <td>1.000690</td>\n",
       "      <td>0.999311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70.244577</td>\n",
       "      <td>70.295367</td>\n",
       "      <td>1.000723</td>\n",
       "      <td>0.999277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.544483</td>\n",
       "      <td>31.566211</td>\n",
       "      <td>1.000689</td>\n",
       "      <td>0.999312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.817506</td>\n",
       "      <td>11.825441</td>\n",
       "      <td>1.000671</td>\n",
       "      <td>0.999329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.029027</td>\n",
       "      <td>5.032150</td>\n",
       "      <td>1.000621</td>\n",
       "      <td>0.999379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.290481</td>\n",
       "      <td>2.291755</td>\n",
       "      <td>1.000556</td>\n",
       "      <td>0.999444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.123706</td>\n",
       "      <td>1.124275</td>\n",
       "      <td>1.000506</td>\n",
       "      <td>0.999494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.565894</td>\n",
       "      <td>0.566158</td>\n",
       "      <td>1.000466</td>\n",
       "      <td>0.999534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.245723</td>\n",
       "      <td>0.245824</td>\n",
       "      <td>1.000415</td>\n",
       "      <td>0.999586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.054556</td>\n",
       "      <td>0.054573</td>\n",
       "      <td>1.000327</td>\n",
       "      <td>0.999673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pine           vp ratio vp/ratio ratio pine/vp\n",
       "                0            0              0             0\n",
       "data                                                       \n",
       "1     5611.439817  5616.248344       1.000857      0.999144\n",
       "2     3154.767573  3157.559479       1.000885      0.999116\n",
       "3     1194.467638  1195.354244       1.000742      0.999258\n",
       "4      515.187956   515.574565       1.000750      0.999250\n",
       "5      246.767542   246.941654       1.000706      0.999295\n",
       "6      126.644243   126.731587       1.000690      0.999311\n",
       "7       70.244577    70.295367       1.000723      0.999277\n",
       "8       31.544483    31.566211       1.000689      0.999312\n",
       "9       11.817506    11.825441       1.000671      0.999329\n",
       "10       5.029027     5.032150       1.000621      0.999379\n",
       "11       2.290481     2.291755       1.000556      0.999444\n",
       "12       1.123706     1.124275       1.000506      0.999494\n",
       "13       0.565894     0.566158       1.000466      0.999534\n",
       "14       0.245723     0.245824       1.000415      0.999586\n",
       "15       0.054556     0.054573       1.000327      0.999673"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try to get a prediction out of it\n",
    "pdf = API.pdf(pdf=\"NNPDF40_nnlo_as_01180\")\n",
    "new_cp = central_predictions(ds, pdf)\n",
    "cp = central_predictions(old_ds, pdf)\n",
    "pd.concat([new_cp, cp, cp/new_cp, new_cp/cp], axis=1, keys=[\"pine\", \"vp\", \"ratio vp/ratio\", \"ratio pine/vp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0905d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pine_fkspec = ds.fkspecs[0]\n",
    "old_fkspec = old_ds.fkspecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd19243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.4.0 loading all 101 PDFs in set NNPDF40_nnlo_as_01180\n",
      "NNPDF40_nnlo_as_01180, version 1; 101 PDF members\n",
      "[0.5982222  0.53206434 0.37638204 0.13101867 0.59686953 0.51087613\n",
      " 0.35425162 0.12440635 0.23262168 0.1988827  0.14422644 0.05755101\n",
      " 0.01654738 0.01469668 0.01335605 0.00692297]\n"
     ]
    }
   ],
   "source": [
    "import pineappl\n",
    "pines = [pineappl.fk_table.FkTable.read(i.as_posix()) for i in pine_fkspec.fkpath]\n",
    "# Inspect the pineappl prediction\n",
    "res_pine = []\n",
    "pp = pines[0]\n",
    "lpdf = pdf.load()\n",
    "\n",
    "for p in pines:\n",
    "    res_pine.append(p.convolute_with_one(2212, lpdf.central_member.xfxQ2))\n",
    "total_pine = np.concatenate(res_pine)\n",
    "print(total_pine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a77d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the content of the old fktables, remove the cfactor for now\n",
    "from validphys.fkparser import load_fktable\n",
    "old_fkspec.cfactors = False\n",
    "old_fktabledata = load_fktable(old_fkspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab604e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadronic?: True\n",
      "Q: 1.65\n",
      "n: 16\n",
      "xgrid shape: (40,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"hadronic?: {old_fktabledata.hadronic}\")\n",
    "print(f\"Q: {old_fktabledata.Q0}\")\n",
    "print(f\"n: {old_fktabledata.ndata}\")\n",
    "print(f\"xgrid shape: {old_fktabledata.xgrid.shape}\")\n",
    "#old_fktabledata.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62709983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read the metadata that vp `FKTableData` needs and that all subgrids share\n",
    "Q0 = np.sqrt(pp.muf2())\n",
    "xgrid = pp.x_grid()\n",
    "# Hadronic means in practice that not all luminosity combinations are just electron X proton\n",
    "hadronic = not all(-11 in i for i in pp.lumi())\n",
    "# Now prepare the concatenation of grids\n",
    "fktables = []\n",
    "for p in pines:\n",
    "    tmp = p.table().T/p.bin_normalizations()\n",
    "    fktables.append(tmp.T)\n",
    "fktable = np.concatenate(fktables, axis=0)\n",
    "ndata = fktable.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d5a130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 21),\n",
       " (100, 203),\n",
       " (100, 208),\n",
       " (100, 200),\n",
       " (100, 103),\n",
       " (100, 108),\n",
       " (100, 115),\n",
       " (21, 21),\n",
       " (21, 203),\n",
       " (21, 208),\n",
       " (21, 200),\n",
       " (21, 103),\n",
       " (21, 108),\n",
       " (21, 115),\n",
       " (21, 100),\n",
       " (200, 203),\n",
       " (200, 208),\n",
       " (203, 203),\n",
       " (203, 208),\n",
       " (203, 200),\n",
       " (203, 103),\n",
       " (203, 108),\n",
       " (203, 115),\n",
       " (203, 100),\n",
       " (208, 208),\n",
       " (208, 200),\n",
       " (208, 103),\n",
       " (208, 108),\n",
       " (208, 115),\n",
       " (208, 100),\n",
       " (200, 200),\n",
       " (200, 103),\n",
       " (200, 108),\n",
       " (200, 115),\n",
       " (200, 100),\n",
       " (103, 103),\n",
       " (103, 108),\n",
       " (103, 115),\n",
       " (103, 100),\n",
       " (108, 108),\n",
       " (108, 115),\n",
       " (108, 100),\n",
       " (115, 115),\n",
       " (115, 100),\n",
       " (100, 100)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.lumi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95c8549",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99782f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to join the fktable, luminosity and xgrid into a pandas dataframe\n",
    "# keeping compatibility with validphys and, hopefully, 50% of my own sanity\n",
    "\n",
    "# Step 1), make the luminosity into a 14x14 mask for the evolution basis\n",
    "eko_numbering_scheme = (22, 100, 21, 200, 203, 208, 215, 224, 235, 103, 108, 115, 124, 135)\n",
    "# note that this is the same ordering that was used in fktables\n",
    "co = []\n",
    "for i, j in pp.lumi():\n",
    "    # Ask where this would fall in a 14x14 matrix\n",
    "    idx = eko_numbering_scheme.index(i)\n",
    "    jdx = eko_numbering_scheme.index(j)\n",
    "    co.append(idx*14 + jdx)\n",
    "    \n",
    "# Step 2) prepare the indices for the dataframe\n",
    "xi = np.arange(len(xgrid))\n",
    "ni = np.arange(ndata)\n",
    "mi = pd.MultiIndex.from_product([ni, xi, xi], names=[\"data\", \"x1\", \"x2\"])\n",
    "\n",
    "# Step 3) Now play with the array until we flatten it in the right way?\n",
    "# The fktables for pineappl have this extra factor of x...\n",
    "# The output of pineappl is (ndata, flavours, x, x)\n",
    "lf = len(co)\n",
    "xfktable = fktable.reshape(ndata, lf, -1)/(xgrid[:,None]*xgrid[None,:]).flatten()\n",
    "fkmod = np.moveaxis(xfktable, 1, -1)\n",
    "fkframe = fkmod.reshape(-1, lf)\n",
    "\n",
    "# Uff, big\n",
    "df = pd.DataFrame(fkframe, index=mi, columns=co)\n",
    "\n",
    "from validphys.convolution import central_hadron_predictions\n",
    "from validphys.coredata import FKTableData\n",
    "fk = FKTableData(sigma=df, ndata=ndata,  Q0=Q0, metadata=None, hadronic=True, xgrid=xgrid)\n",
    "central_hadron_predictions(fk, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e323f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a luminosity tensor and check that the results are correct\n",
    "from validphys.pdfbases import evolution\n",
    "\n",
    "evol_basis = (\n",
    "    \"photon\",\n",
    "    \"singlet\",\n",
    "    \"g\",\n",
    "    \"V\",\n",
    "    \"V3\",\n",
    "    \"V8\",\n",
    "    \"V15\",\n",
    "    \"V24\",\n",
    "    \"V35\",\n",
    "    \"T3\",\n",
    "    \"T8\",\n",
    "    \"T15\",\n",
    "    \"T24\",\n",
    "    \"T35\",\n",
    ")\n",
    "total_pdf = evolution.grid_values(pdf, evol_basis, xgrid, [Q0]).squeeze()[0]/xgrid\n",
    "print(total_pdf.shape)\n",
    "lumi = np.einsum('ij,kl->ikjl', total_pdf, total_pdf)\n",
    "lumi_masked = lumi[flavour_map]\n",
    "print(fktable.shape)\n",
    "print(lumi_masked.shape)\n",
    "res = np.einsum('ijkl,jkl->i', fktable, lumi_masked)\n",
    "#pd.concat([pd.DataFrame(res), cp, pd.DataFrame(res)/cp,  ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfktable.reshape(48,91,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validphys.fkparser import open_fkpath, _parse_string, _parse_header, _build_sigma\n",
    "from validphys.fkparser import _parse_flavour_map, _parse_hadronic_fast_kernel\n",
    "try:\n",
    "    f.close()\n",
    "except:\n",
    "    pass\n",
    "f = open_fkpath(old_fkspec.fkpath)\n",
    "line_and_stream = enumerate(f, start=1)\n",
    "lineno, header = next(line_and_stream)\n",
    "res = {}\n",
    "while True:\n",
    "    marker, header_name = _parse_header(lineno, header)\n",
    "    if header_name == \"FastKernel\":\n",
    "        break\n",
    "    if header_name == \"FlavourMap\":\n",
    "        out, lineno, header = _parse_flavour_map(line_and_stream)\n",
    "    else:\n",
    "        out, lineno, header = _parse_string(line_and_stream)\n",
    "    res[header_name] = out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd41ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"FlavourMap\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_hate_pandas = _parse_hadronic_fast_kernel(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde47252",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_hate_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fktabledata.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910dbb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnvortex",
   "language": "python",
   "name": "nnvortex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
