{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cceb1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from validphys.loader import _get_nnpdf_profile\n",
    "from validphys.api import API\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from validphys.convolution import central_predictions\n",
    "\n",
    "profile = _get_nnpdf_profile()\n",
    "yaml_db = Path(profile[\"data_path\"]) / \"yamldb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1eb8f3",
   "metadata": {},
   "source": [
    "The `yaml_db` folder is a temporary thing as it contains files that look like:\n",
    "\n",
    "```yaml\n",
    "conversion_factor: 1.0\n",
    "operands:\n",
    "- - NMC_NC_EM_D_F2\n",
    "- - NMC_NC_EM_P_F2\n",
    "operation: RATIO\n",
    "target_dataset: NMCPD\n",
    "```\n",
    "\n",
    "This information will eventually be part of the new commondata format of course.\n",
    "\n",
    "The `operation` is applied to the first level of the list while the second level is just concatenated. This is necessary since `pineappl` fktables might contain one layer of concatenation which is already done for the \"classic\" fktables.\n",
    "\n",
    "The `pineappl` fktables will live inside the appropiate `theory_xxx` folder `/pineappls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051581e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test them all\n",
    "if False:\n",
    "    from yaml import safe_load\n",
    "    pdf = API.pdf(pdf=\"NNPDF40_nnlo_as_01180\")\n",
    "    all_res = []\n",
    "    # Reference here a NNPDF40 runcard to read up all datasets\n",
    "    nnpdf40_runcard = safe_load(Path(\"/home/juacrumar/NNPDF-testing/nnpdf/n3fit/NNPDF40_with_pineappl.yml\").read_text())\n",
    "    #nnpdf40_runcard = safe_load(Path(\"/mount/storage/Academic_Workspace/NNPDF/source/nnpdf/n3fit/NNPDF40_with_pineappl.yml\").read_text())\n",
    "    for d in nnpdf40_runcard[\"dataset_inputs\"]:\n",
    "        target_ds = d[\"dataset\"]\n",
    "        #if any(skipthis in target_ds for skipthis in [\"HERA\", \"NMC\", \"NTV\", \"CHORUS\", \"SLAC\", \"BCD\"]):\n",
    "        #    continue\n",
    "        print(target_ds)\n",
    "        cfac = d.get(\"cfac\", [])\n",
    "        old_ds = API.dataset(dataset_input={\"dataset\": target_ds, \"cfac\": cfac + [\"oldmode\"]}, theoryid=200, use_cuts=\"internal\")\n",
    "        ds = API.dataset(dataset_input={\"dataset\": target_ds, \"cfac\": cfac}, theoryid=200, use_cuts=\"internal\")\n",
    "        new_cp = central_predictions(ds, pdf)\n",
    "        cp = central_predictions(old_ds, pdf)\n",
    "        all_res.append(pd.concat([new_cp, cp, new_cp/cp], axis=1, keys=[\"vp\", \"pine\", f\"ratio {target_ds}, {cfac}\"]))\n",
    "        \n",
    "    for i in all_res:\n",
    "        mean_ratio = i[i.columns[2]].mean()\n",
    "        if not (0.95 < mean_ratio < 1.05):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d88771",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ds = \"ATLAS_WP_JET_8TEV_PT\"\n",
    "cfac = [\"QCD\"] # [\"NRM\"]\n",
    "old_ds = API.dataset(dataset_input={\"dataset\": target_ds, \"cfac\": cfac + [\"oldmode\"]}, theoryid=200, use_cuts=\"internal\")\n",
    "ds = API.dataset(dataset_input={\"dataset\": target_ds, \"cfac\": cfac}, theoryid=200, use_cuts=\"internal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a571e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Reading COMMONDATA for Dataset: ATLAS_WP_JET_8TEV_PT\n",
      "nData: 16 nSys: 124\n",
      "-- COMMONDATA Files for ATLAS_WP_JET_8TEV_PT successfully read.\n",
      "\n",
      "> \u001b[0;32m/home/juacrumar/NNPDF-testing/nnpdf/validphys2/src/validphys/pineparser.py\u001b[0m(243)\u001b[0;36mpineappl_reader\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    242 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 243 \u001b[0;31m    \u001b[0mpine_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    244 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/juacrumar/NNPDF-testing/nnpdf/validphys2/src/validphys/pineparser.py\u001b[0m(246)\u001b[0;36mpineappl_reader\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    245 \u001b[0;31m    \u001b[0;31m# Is it hadronic? (at the moment only hadronic and DIS are considered)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 246 \u001b[0;31m    \u001b[0mhadronic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpine_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_state_1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpine_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_state_2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    247 \u001b[0;31m    \u001b[0;31m# Sanity check (in case at some point we start fitting things that are not protons)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> pine_rep\n",
      "<pineappl.fk_table.FkTable object at 0x7f1ae0a6f6d0>\n",
      "ipdb> pine_rep.metadata\n",
      "*** AttributeError: 'builtins.PyFkTable' object has no attribute 'metadata'\n",
      "ipdb> dir(pine_rep)\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_raw', 'raw', 'read']\n",
      "ipdb> pine_rep._raw\n",
      "<builtins.PyFkTable object at 0x5597fa22c4c0>\n",
      "ipdb> dir(pine_rep._raw)\n",
      "['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'bin_dimensions', 'bin_left', 'bin_normalizations', 'bin_right', 'bins', 'convolute_with_one', 'key_values', 'lumi', 'muf2', 'read', 'table', 'write', 'write_lz4', 'x_grid']\n",
      "ipdb> dir(pine_rep._raw.key_values)\n",
      "['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__self__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__text_signature__']\n",
      "ipdb> print(pine_rep._raw.key_values.keys())\n",
      "*** AttributeError: 'builtin_function_or_method' object has no attribute 'keys'\n",
      "ipdb> print(pine_rep._raw.key_values())\n",
      "{'lumi_id_types': 'evol', 'pineappl_gitversion': 'v0.5.0-beta.3-18-g32e63c9', 'initial_state_2': '2212', 'initial_state_1': '2212'}\n"
     ]
    }
   ],
   "source": [
    "# Let's try to get a prediction out of it\n",
    "pdf = API.pdf(pdf=\"NNPDF40_nnlo_as_01180\")\n",
    "new_cp = central_predictions(ds, pdf)\n",
    "cp = central_predictions(old_ds, pdf)\n",
    "pd.concat([new_cp, cp, cp/new_cp, new_cp/cp], axis=1, keys=[\"pine\", \"vp\", \"ratio vp/ratio\", \"ratio pine/vp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0905d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pine_fkspec = ds.fkspecs[0]\n",
    "old_fkspec = old_ds.fkspecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd19243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.4.0 loading all 101 PDFs in set NNPDF40_nnlo_as_01180\n",
      "NNPDF40_nnlo_as_01180, version 1; 101 PDF members\n",
      "[0.5982222  0.53206434 0.37638204 0.13101867 0.59686953 0.51087613\n",
      " 0.35425162 0.12440635 0.23262168 0.1988827  0.14422644 0.05755101\n",
      " 0.01654738 0.01469668 0.01335605 0.00692297]\n"
     ]
    }
   ],
   "source": [
    "import pineappl\n",
    "pines = [pineappl.fk_table.FkTable.read(i.as_posix()) for i in pine_fkspec.fkpath]\n",
    "# Inspect the pineappl prediction\n",
    "res_pine = []\n",
    "pp = pines[0]\n",
    "lpdf = pdf.load()\n",
    "\n",
    "for p in pines:\n",
    "    res_pine.append(p.convolute_with_one(2212, lpdf.central_member.xfxQ2))\n",
    "total_pine = np.concatenate(res_pine)\n",
    "print(total_pine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a77d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the content of the old fktables, remove the cfactor for now\n",
    "from validphys.fkparser import load_fktable\n",
    "old_fkspec.cfactors = False\n",
    "old_fktabledata = load_fktable(old_fkspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab604e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadronic?: True\n",
      "Q: 1.65\n",
      "n: 16\n",
      "xgrid shape: (40,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"hadronic?: {old_fktabledata.hadronic}\")\n",
    "print(f\"Q: {old_fktabledata.Q0}\")\n",
    "print(f\"n: {old_fktabledata.ndata}\")\n",
    "print(f\"xgrid shape: {old_fktabledata.xgrid.shape}\")\n",
    "#old_fktabledata.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62709983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read the metadata that vp `FKTableData` needs and that all subgrids share\n",
    "Q0 = np.sqrt(pp.muf2())\n",
    "xgrid = pp.x_grid()\n",
    "# Hadronic means in practice that not all luminosity combinations are just electron X proton\n",
    "hadronic = not all(-11 in i for i in pp.lumi())\n",
    "# Now prepare the concatenation of grids\n",
    "fktables = []\n",
    "for p in pines:\n",
    "    tmp = p.table().T/p.bin_normalizations()\n",
    "    fktables.append(tmp.T)\n",
    "fktable = np.concatenate(fktables, axis=0)\n",
    "ndata = fktable.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d5a130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 21),\n",
       " (100, 203),\n",
       " (100, 208),\n",
       " (100, 200),\n",
       " (100, 103),\n",
       " (100, 108),\n",
       " (100, 115),\n",
       " (21, 21),\n",
       " (21, 203),\n",
       " (21, 208),\n",
       " (21, 200),\n",
       " (21, 103),\n",
       " (21, 108),\n",
       " (21, 115),\n",
       " (21, 100),\n",
       " (200, 203),\n",
       " (200, 208),\n",
       " (203, 203),\n",
       " (203, 208),\n",
       " (203, 200),\n",
       " (203, 103),\n",
       " (203, 108),\n",
       " (203, 115),\n",
       " (203, 100),\n",
       " (208, 208),\n",
       " (208, 200),\n",
       " (208, 103),\n",
       " (208, 108),\n",
       " (208, 115),\n",
       " (208, 100),\n",
       " (200, 200),\n",
       " (200, 103),\n",
       " (200, 108),\n",
       " (200, 115),\n",
       " (200, 100),\n",
       " (103, 103),\n",
       " (103, 108),\n",
       " (103, 115),\n",
       " (103, 100),\n",
       " (108, 108),\n",
       " (108, 115),\n",
       " (108, 100),\n",
       " (115, 115),\n",
       " (115, 100),\n",
       " (100, 100)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.lumi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95c8549",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99782f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to join the fktable, luminosity and xgrid into a pandas dataframe\n",
    "# keeping compatibility with validphys and, hopefully, 50% of my own sanity\n",
    "\n",
    "# Step 1), make the luminosity into a 14x14 mask for the evolution basis\n",
    "eko_numbering_scheme = (22, 100, 21, 200, 203, 208, 215, 224, 235, 103, 108, 115, 124, 135)\n",
    "# note that this is the same ordering that was used in fktables\n",
    "co = []\n",
    "for i, j in pp.lumi():\n",
    "    # Ask where this would fall in a 14x14 matrix\n",
    "    idx = eko_numbering_scheme.index(i)\n",
    "    jdx = eko_numbering_scheme.index(j)\n",
    "    co.append(idx*14 + jdx)\n",
    "    \n",
    "# Step 2) prepare the indices for the dataframe\n",
    "xi = np.arange(len(xgrid))\n",
    "ni = np.arange(ndata)\n",
    "mi = pd.MultiIndex.from_product([ni, xi, xi], names=[\"data\", \"x1\", \"x2\"])\n",
    "\n",
    "# Step 3) Now play with the array until we flatten it in the right way?\n",
    "# The fktables for pineappl have this extra factor of x...\n",
    "# The output of pineappl is (ndata, flavours, x, x)\n",
    "lf = len(co)\n",
    "xfktable = fktable.reshape(ndata, lf, -1)/(xgrid[:,None]*xgrid[None,:]).flatten()\n",
    "fkmod = np.moveaxis(xfktable, 1, -1)\n",
    "fkframe = fkmod.reshape(-1, lf)\n",
    "\n",
    "# Uff, big\n",
    "df = pd.DataFrame(fkframe, index=mi, columns=co)\n",
    "\n",
    "from validphys.convolution import central_hadron_predictions\n",
    "from validphys.coredata import FKTableData\n",
    "fk = FKTableData(sigma=df, ndata=ndata,  Q0=Q0, metadata=None, hadronic=True, xgrid=xgrid)\n",
    "central_hadron_predictions(fk, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e323f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a luminosity tensor and check that the results are correct\n",
    "from validphys.pdfbases import evolution\n",
    "\n",
    "evol_basis = (\n",
    "    \"photon\",\n",
    "    \"singlet\",\n",
    "    \"g\",\n",
    "    \"V\",\n",
    "    \"V3\",\n",
    "    \"V8\",\n",
    "    \"V15\",\n",
    "    \"V24\",\n",
    "    \"V35\",\n",
    "    \"T3\",\n",
    "    \"T8\",\n",
    "    \"T15\",\n",
    "    \"T24\",\n",
    "    \"T35\",\n",
    ")\n",
    "total_pdf = evolution.grid_values(pdf, evol_basis, xgrid, [Q0]).squeeze()[0]/xgrid\n",
    "print(total_pdf.shape)\n",
    "lumi = np.einsum('ij,kl->ikjl', total_pdf, total_pdf)\n",
    "lumi_masked = lumi[flavour_map]\n",
    "print(fktable.shape)\n",
    "print(lumi_masked.shape)\n",
    "res = np.einsum('ijkl,jkl->i', fktable, lumi_masked)\n",
    "#pd.concat([pd.DataFrame(res), cp, pd.DataFrame(res)/cp,  ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfktable.reshape(48,91,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validphys.fkparser import open_fkpath, _parse_string, _parse_header, _build_sigma\n",
    "from validphys.fkparser import _parse_flavour_map, _parse_hadronic_fast_kernel\n",
    "try:\n",
    "    f.close()\n",
    "except:\n",
    "    pass\n",
    "f = open_fkpath(old_fkspec.fkpath)\n",
    "line_and_stream = enumerate(f, start=1)\n",
    "lineno, header = next(line_and_stream)\n",
    "res = {}\n",
    "while True:\n",
    "    marker, header_name = _parse_header(lineno, header)\n",
    "    if header_name == \"FastKernel\":\n",
    "        break\n",
    "    if header_name == \"FlavourMap\":\n",
    "        out, lineno, header = _parse_flavour_map(line_and_stream)\n",
    "    else:\n",
    "        out, lineno, header = _parse_string(line_and_stream)\n",
    "    res[header_name] = out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd41ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"FlavourMap\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_hate_pandas = _parse_hadronic_fast_kernel(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde47252",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_hate_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fktabledata.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910dbb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devnnpdf",
   "language": "python",
   "name": "devnnpdf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
